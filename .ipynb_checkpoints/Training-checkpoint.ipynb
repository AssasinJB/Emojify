{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "val_dir = 'data/test'\n",
    "train_data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "val_data_gen = ImageDataGenerator(rescale = 1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(train_dir,\n",
    "                                                    target_size = (48,48),\n",
    "                                                    batch_size = 64,\n",
    "                                                     color_mode = \"grayscale\",\n",
    "                                                     class_mode = 'categorical'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = val_data_gen.flow_from_directory(val_dir,\n",
    "                                                       target_size = (48,48),\n",
    "                                                       batch_size = 64,\n",
    "                                                       color_mode='grayscale',\n",
    "                                                       class_mode = 'categorical'\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the cnn network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(32,kernel_size =(3,3),activation = 'relu',input_shape = (48,48,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(64,kernel_size =(3,3),activation = 'relu',input_shape = (48,48,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128,kernel_size = (3,3),activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "emotion_model.add(Conv2D(128,kernel_size = (3,3),activation = 'relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024,activation = 'relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and train the model\n",
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr = 0.0001,decay = 1e-6),\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 22s 48ms/step - loss: 1.8074 - accuracy: 0.2539 - val_loss: 1.7255 - val_accuracy: 0.3273\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 11s 26ms/step - loss: 1.6391 - accuracy: 0.3600 - val_loss: 1.5511 - val_accuracy: 0.4125\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.5325 - accuracy: 0.4071 - val_loss: 1.4674 - val_accuracy: 0.4415\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.4610 - accuracy: 0.4405 - val_loss: 1.4355 - val_accuracy: 0.4533\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.4053 - accuracy: 0.4656 - val_loss: 1.3593 - val_accuracy: 0.4823\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 1.3563 - accuracy: 0.4823 - val_loss: 1.3216 - val_accuracy: 0.4996\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.3140 - accuracy: 0.4987 - val_loss: 1.2857 - val_accuracy: 0.5138\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.2712 - accuracy: 0.5165 - val_loss: 1.2556 - val_accuracy: 0.5266\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.2399 - accuracy: 0.5287 - val_loss: 1.2293 - val_accuracy: 0.5340\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.2074 - accuracy: 0.5422 - val_loss: 1.2063 - val_accuracy: 0.5442\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 1.1711 - accuracy: 0.5575 - val_loss: 1.1847 - val_accuracy: 0.5488\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.1432 - accuracy: 0.5674 - val_loss: 1.1735 - val_accuracy: 0.5573\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.1240 - accuracy: 0.5784 - val_loss: 1.1547 - val_accuracy: 0.5667\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.0980 - accuracy: 0.5858 - val_loss: 1.1418 - val_accuracy: 0.5758\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.0706 - accuracy: 0.6002 - val_loss: 1.1430 - val_accuracy: 0.5675\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 1.0485 - accuracy: 0.6112 - val_loss: 1.1254 - val_accuracy: 0.5749\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.0265 - accuracy: 0.6187 - val_loss: 1.1213 - val_accuracy: 0.5770\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.0034 - accuracy: 0.6277 - val_loss: 1.1114 - val_accuracy: 0.5816\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.9820 - accuracy: 0.6349 - val_loss: 1.1028 - val_accuracy: 0.5878\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.9631 - accuracy: 0.6413 - val_loss: 1.0947 - val_accuracy: 0.5918\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 0.9445 - accuracy: 0.6541 - val_loss: 1.1011 - val_accuracy: 0.5882\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.9119 - accuracy: 0.6611 - val_loss: 1.0859 - val_accuracy: 0.5949\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.8896 - accuracy: 0.6706 - val_loss: 1.0806 - val_accuracy: 0.6032\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.8701 - accuracy: 0.6811 - val_loss: 1.0749 - val_accuracy: 0.6027\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.8477 - accuracy: 0.6895 - val_loss: 1.0737 - val_accuracy: 0.6042\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 0.8293 - accuracy: 0.6989 - val_loss: 1.0688 - val_accuracy: 0.6078\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.8006 - accuracy: 0.7078 - val_loss: 1.0837 - val_accuracy: 0.6055\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.7817 - accuracy: 0.7146 - val_loss: 1.0775 - val_accuracy: 0.6087\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 0.7591 - accuracy: 0.7229 - val_loss: 1.0792 - val_accuracy: 0.6115\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 0.7332 - accuracy: 0.7332 - val_loss: 1.0746 - val_accuracy: 0.6115\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 12s 28ms/step - loss: 0.7170 - accuracy: 0.7399 - val_loss: 1.0752 - val_accuracy: 0.6110\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.6930 - accuracy: 0.7459 - val_loss: 1.0957 - val_accuracy: 0.6066\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.6652 - accuracy: 0.7550 - val_loss: 1.0896 - val_accuracy: 0.6136\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 0.6440 - accuracy: 0.7659 - val_loss: 1.0900 - val_accuracy: 0.6105\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 13s 30ms/step - loss: 0.6268 - accuracy: 0.7729 - val_loss: 1.1090 - val_accuracy: 0.6198\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 0.6063 - accuracy: 0.7797 - val_loss: 1.1022 - val_accuracy: 0.6165\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.5831 - accuracy: 0.7874 - val_loss: 1.1177 - val_accuracy: 0.6126\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 0.5728 - accuracy: 0.7906 - val_loss: 1.1160 - val_accuracy: 0.6215\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.5500 - accuracy: 0.8005 - val_loss: 1.1235 - val_accuracy: 0.6170\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 0.5347 - accuracy: 0.8068 - val_loss: 1.1204 - val_accuracy: 0.6226\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 0.5148 - accuracy: 0.8162 - val_loss: 1.1339 - val_accuracy: 0.6208\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.4993 - accuracy: 0.8176 - val_loss: 1.1361 - val_accuracy: 0.6222\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 0.4829 - accuracy: 0.8261 - val_loss: 1.1396 - val_accuracy: 0.6179\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.4675 - accuracy: 0.8297 - val_loss: 1.1348 - val_accuracy: 0.6200\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 0.4546 - accuracy: 0.8346 - val_loss: 1.1482 - val_accuracy: 0.6207\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.4384 - accuracy: 0.8401 - val_loss: 1.1688 - val_accuracy: 0.6218\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.4231 - accuracy: 0.8488 - val_loss: 1.1841 - val_accuracy: 0.6237\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.4089 - accuracy: 0.8549 - val_loss: 1.1838 - val_accuracy: 0.6225\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 0.4008 - accuracy: 0.8577 - val_loss: 1.1881 - val_accuracy: 0.6212\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 13s 30ms/step - loss: 0.3893 - accuracy: 0.8595 - val_loss: 1.1738 - val_accuracy: 0.6200\n"
     ]
    }
   ],
   "source": [
    "emotional_model_info = emotion_model.fit(\n",
    "train_generator,steps_per_epoch= 28709//64,\n",
    "epochs = 50,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = 7178//64\n",
    ")\n",
    "# emotional_model_info = emotion_model.fit(\n",
    "# train_generator,\n",
    "# epochs = 50,\n",
    "# validation_data = validation_generator,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15885264226605189957\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2312281272197040048\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9303167960032795397\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('C:\\\\Users\\Bhojwani\\Anaconda3\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
